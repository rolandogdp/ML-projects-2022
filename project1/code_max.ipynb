{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40da9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a374a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, tx, w):\n",
    "    \"\"\"Compute MSE at w\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D+1)\n",
    "        w: numpy array of shape=(D+1, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        Returns the mean square error at w for input tx and output y\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return np.mean(e**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ccc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D+1)\n",
    "        w: numpy array of shape=(D+1, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (D+1, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return -tx.T.dot(e)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0e260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradientSGD(y, tx, w):\n",
    "    \"\"\"Computes the gradient SGD at w for batches of size one.\n",
    "        \n",
    "    Args:\n",
    "        y: a number\n",
    "        tx: numpy array of shape=(D+1, )\n",
    "        w: numpy array of shape=(D+1, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (D+1, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return -tx.T.dot(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f49f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm for least squares.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D+1)\n",
    "        initial_w: numpy array of shape=(D+1, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        w: the model parameter as numpy arrays of shape (2, ), for the last iteration of GD \n",
    "        loss: the loss value corresponding to w\n",
    "    \"\"\"\n",
    "    \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "\n",
    "    loss = MSE(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4fe9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample(y, tx):\n",
    "    \"\"\"get a random sample of (y, tx)\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D+1)\n",
    "        \n",
    "    Returns:\n",
    "        y_sample: a random sample of y as a number\n",
    "        tx_sample: a random sample of tx as numpy arrays of shape (D+1, )\n",
    "    \"\"\"\n",
    "    random_sample_index = np.random.randint(len(y))\n",
    "    y_sample  = y [random_sample_index]\n",
    "    tx_sample = tx[random_sample_index]\n",
    "    return y_sample, tx_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d17b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent (SGD) algorithm for least squares using batches of size one.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D+1)\n",
    "        initial_w: numpy array of shape=(D+1, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        w: the model parameter as numpy arrays of shape (2, ), for the last iteration of SGD \n",
    "        loss: the loss value corresponding to w\n",
    "    \"\"\"\n",
    "    \n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        y_sample, tx_sample = get_random_sample(y, tx)\n",
    "        grad = compute_gradientSGD(y_sample, tx_sample, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "    loss = MSE(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391aac1",
   "metadata": {},
   "source": [
    "Let's try to use the QR decomposition for more robust solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6b00214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y : np.array, tx : np.array):\n",
    "    \"\"\"Calculate the least squares solution.\n",
    "       returns mse, and optimal weights.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        loss: loss value as a float\n",
    "    \"\"\"\n",
    "    \n",
    "    Q, R = np.linalg.qr(tx)\n",
    "    w = np.linalg.solve(R, Q.T.dot(y))\n",
    "    loss = MSE(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd3ec4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y : np.array, tx: np.array , lambda_):\n",
    "    \"\"\"Calculate the least squares solution with regularization parameter.\n",
    "       returns mse, and optimal weights.\n",
    "       \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        lambda_: the regularization parameter as a scalar.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        loss: loss value as a float\n",
    "    \"\"\"\n",
    "    \n",
    "    D = tx.shape[1]\n",
    "    lambda_I = np.eye(D) * np.sqrt(2*len(y)*lambda_)\n",
    "    tx_expended = np.append(tx, lambda_I, axis=0)\n",
    "    y_expended  = np.append(y, np.zeros(D))\n",
    "    \n",
    "    Q, R = np.linalg.qr(tx_expended)\n",
    "    w = np.linalg.solve(R, Q.T.dot(y_expended))\n",
    "    loss = MSE(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfc4a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_rolan(y : np.array, tx: np.array , lambda_: np.array ):\n",
    "    \"\"\"Ridge regression implementation using linear solver from numpy instead of inverse for performance reasons.\n",
    "        (X.T*X + lambda*I)*w = X.t*y\n",
    "    Args:\n",
    "        y (np.array): The target values Y\n",
    "        tx (np.array): The input array x padded with a vector of ones for the bias.\n",
    "        lambda_ (np.array): The regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.array, float]: Weights w as a numpy array, loss value as a float\n",
    "    \"\"\"\n",
    "\n",
    "    w = np.linalg.solve(tx.T.dot(tx) +  2*tx.shape[0]*lambda_ * np.eye(tx.shape[1]), tx.T.dot(y)) \n",
    "    loss = ((y-w.dot(tx.T))**2).mean(axis=0)\n",
    "    \n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca3393d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_cours(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22787c7",
   "metadata": {},
   "source": [
    "# Sandbox for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0d502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54.7        15.08492696  3.55532095]\n"
     ]
    }
   ],
   "source": [
    "x = [[12,16,71,99,45,27,80,58,4,50],\n",
    "     [35,78,73,3,55,43,56,98,32,40]]\n",
    "x = (x-np.mean(x, axis=1).reshape(2,1))/np.std(x, axis=1).reshape(2,1)\n",
    "y = [56,22,37,78,83,55,70,94,12,40]\n",
    "x = np.array(x).T\n",
    "tx = np.insert(x, 0, 1, axis=1)\n",
    "y = np.array(y)\n",
    "linreg = LinearRegression().fit(x,y)\n",
    "initial_w = np.array([0,0,0])\n",
    "w = initial_w\n",
    "\n",
    "\n",
    "print(np.append(np.array(linreg.intercept_), linreg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2965108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3646.7\n",
      "3646.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(mean_squared_error(y, tx.dot(w)))\n",
    "print(MSE(y, tx, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b05fdab",
   "metadata": {},
   "source": [
    "### Testing for least Squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b46ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([54.69854709, 15.08425522,  3.55477245]), 423.1618830220783)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares_GD(y, tx, initial_w, 100, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6f09ad",
   "metadata": {},
   "source": [
    "### Testing for least Squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd1795e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([54.69782865, 15.08606612,  3.60861244]), 423.1647163135561)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares_SGD(y, tx, initial_w, 100000, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fee9b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([54.7       , 15.08492696,  3.55532095]), 423.16188021914616)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares(y, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc8c8de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([27.35      ,  7.45729914,  1.4666486 ]), 1231.1292309891364)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_regression(y, tx , lambda_=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41751f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.23333333,  4.95639555,  0.90978386])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_regression_cours(y, tx, lambda_=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdaaeb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([18.23333333,  4.95639555,  0.90978386]), 1858.1954078615443)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_regression_rolan(y, tx, lambda_=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8fbafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
