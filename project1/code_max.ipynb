{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40da9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a374a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, tx, w):\n",
    "    \"\"\"Compute MSE at w\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D+1)\n",
    "        w: numpy array of shape=(D+1, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        Returns the mean square error at w for input tx and output y\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return np.mean(e**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ccc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D+1)\n",
    "        w: numpy array of shape=(D+1, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (D+1, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return -tx.T.dot(e)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0e260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradientSGD(y, tx, w):\n",
    "    \"\"\"Computes the gradient SGD at w for batches of size one.\n",
    "        \n",
    "    Args:\n",
    "        y: a number\n",
    "        tx: numpy array of shape=(D+1, )\n",
    "        w: numpy array of shape=(D+1, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (D+1, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return -tx.T.dot(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31f49f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm for least squares.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D+1)\n",
    "        initial_w: numpy array of shape=(D+1, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        w: the model parameter as numpy arrays of shape (2, ), for the last iteration of GD \n",
    "        loss: the loss value corresponding to w\n",
    "    \"\"\"\n",
    "    \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "\n",
    "    loss = MSE(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b4fe9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample(y, tx):\n",
    "    \"\"\"get a random sample of (y, tx)\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D+1)\n",
    "        \n",
    "    Returns:\n",
    "        y_sample: a random sample of y as a number\n",
    "        tx_sample: a random sample of tx as numpy arrays of shape (D+1, )\n",
    "    \"\"\"\n",
    "    random_sample_index = np.random.randint(len(y))\n",
    "    y_sample  = y [random_sample_index]\n",
    "    tx_sample = tx[random_sample_index]\n",
    "    return y_sample, tx_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d17b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent (SGD) algorithm for least squares using batches of size one.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,D+1)\n",
    "        initial_w: numpy array of shape=(D+1, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        w: the model parameter as numpy arrays of shape (2, ), for the last iteration of SGD \n",
    "        loss: the loss value corresponding to w\n",
    "    \"\"\"\n",
    "    \n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        y_sample, tx_sample = get_random_sample(y, tx)\n",
    "        grad = compute_gradientSGD(y_sample, tx_sample, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "    loss = MSE(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22787c7",
   "metadata": {},
   "source": [
    "# Sandbox for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c0d502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54.7        15.08492696  3.55532095]\n"
     ]
    }
   ],
   "source": [
    "x = [[12,16,71,99,45,27,80,58,4,50],\n",
    "     [35,78,73,3,55,43,56,98,32,40]]\n",
    "x = (x-np.mean(x, axis=1).reshape(2,1))/np.std(x, axis=1).reshape(2,1)\n",
    "y = [56,22,37,78,83,55,70,94,12,40]\n",
    "x = np.array(x).T\n",
    "tx = np.insert(x, 0, 1, axis=1)\n",
    "y = np.array(y)\n",
    "linreg = LinearRegression().fit(x,y)\n",
    "initial_w = np.array([0,0,0])\n",
    "w = initial_w\n",
    "\n",
    "\n",
    "print(np.append(np.array(linreg.intercept_), linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b05fdab",
   "metadata": {},
   "source": [
    "### Testing for least Squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05b46ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([54.69854709, 15.08425522,  3.55477245]), 423.1618830220783)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares_GD(y, tx, initial_w, 100, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6f09ad",
   "metadata": {},
   "source": [
    "### Testing for least Squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd1795e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([54.8223406 , 14.90603655,  3.58603709]), 423.21068889343735)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares_SGD(y, tx, initial_w, 100000, 0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
