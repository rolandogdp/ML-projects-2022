{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40da9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9c72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(y, tx, initial_w, max_iters, gamma, gradient_func, loss_func):\n",
    "    w = initial_w\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = gradient_func(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "\n",
    "    loss = loss_func(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "def GD_reg(y, tx, initial_w, max_iters, gamma, gradient_func, loss_func, lambda_):\n",
    "    w = initial_w\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        grad = gradient_func(y, tx, w, lambda_)\n",
    "        w = w - gamma * grad\n",
    "\n",
    "    loss = loss_func(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d14637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample(y, tx):\n",
    "    random_sample_index = np.random.randint(len(y))\n",
    "    y_sample  = y [random_sample_index]\n",
    "    tx_sample = tx[random_sample_index]\n",
    "    return y_sample, tx_sample\n",
    "\n",
    "def SGD(y, tx, initial_w, max_iters, gamma, gradient_func, loss_func):\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        y_sample, tx_sample = get_random_sample(y, tx)\n",
    "        grad = gradient_func(y_sample, tx_sample, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "    loss = loss_func(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630bd7d",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a374a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, tx, w):\n",
    "    e = y - tx.dot(w)\n",
    "    return np.mean(e**2)\n",
    "\n",
    "def least_squares_gradient(y, tx, w):\n",
    "    e = y - tx.dot(w)\n",
    "    return -tx.T.dot(e)/y.size\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w, loss = GD(y, tx, initial_w, max_iters, gamma, least_squares_gradient, MSE)\n",
    "    return w, loss\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    w, loss = SGD(y, tx, initial_w, max_iters, gamma, least_squares_gradient, MSE)\n",
    "    return w, loss\n",
    "\n",
    "def least_squares(y : np.array, tx : np.array):\n",
    "    Q, R = np.linalg.qr(tx)\n",
    "    w = np.linalg.solve(R, Q.T.dot(y))\n",
    "    loss = MSE(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e7757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y : np.array, tx: np.array , lambda_):\n",
    "    D = tx.shape[1]\n",
    "    lambda_I = np.eye(D) * np.sqrt(2*len(y)*lambda_)\n",
    "    tx_expended = np.append(tx, lambda_I, axis=0)\n",
    "    y_expended  = np.append(y, np.zeros(D))\n",
    "    \n",
    "    Q, R = np.linalg.qr(tx_expended)\n",
    "    w = np.linalg.solve(R, Q.T.dot(y_expended))\n",
    "    loss = MSE(y, tx, w)\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897ac01",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa9af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    exp_t = np.exp(t)\n",
    "    return exp_t / (1 + exp_t)\n",
    "\n",
    "def logistic_loss(y, tx, w):\n",
    "    xtw = tx.dot(w)\n",
    "    loss = np.sum(np.log(1 + np.exp(xtw))) - y.T.dot(xtw)\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "def logistic_gradient(y, tx, w):\n",
    "    return tx.T.dot(sigmoid(tx.dot(w)) - y)\n",
    "\n",
    "def reg_logistic_gradient(y, tx, w, lambda_):\n",
    "    return logistic_gradient(y, tx, w) + 2 * lambda_ * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea75fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    w, loss = GD(y, tx, initial_w, max_iters, gamma, logistic_gradient, logistic_loss)\n",
    "    return w, loss\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    w, loss = GD_reg(y, tx, initial_w, max_iters, gamma, reg_logistic_gradient, logistic_loss, lambda_)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22787c7",
   "metadata": {},
   "source": [
    "# Sandbox for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0d502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54.7        15.08492696  3.55532095]\n"
     ]
    }
   ],
   "source": [
    "x = [[12,16,71,99,45,27,80,58,4,50],\n",
    "     [35,78,73,3,55,43,56,98,32,40]]\n",
    "x = (x-np.mean(x, axis=1).reshape(2,1))/np.std(x, axis=1).reshape(2,1)\n",
    "y = [56,22,37,78,83,55,70,94,12,40]\n",
    "x = np.array(x).T\n",
    "tx = np.insert(x, 0, 1, axis=1)\n",
    "y = np.array(y)\n",
    "linreg = LinearRegression().fit(x,y)\n",
    "initial_w = np.array([0,0,0])\n",
    "w = initial_w\n",
    "\n",
    "\n",
    "print(np.append(np.array(linreg.intercept_), linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b05fdab",
   "metadata": {},
   "source": [
    "### Testing for least Squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b46ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([54.69854709, 15.08425522,  3.55477245]), 423.1618830220783)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares_GD(y, tx, initial_w, 100, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6f09ad",
   "metadata": {},
   "source": [
    "### Testing for least Squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd1795e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([54.79272918, 14.99698322,  3.65181066]), 423.1889072789986)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares_SGD(y, tx, initial_w, 100000, 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e0ce7",
   "metadata": {},
   "source": [
    "### Testing least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bd6476b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([54.7       , 15.08492696,  3.55532095]), 423.16188021914616)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares(y, tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0085c64",
   "metadata": {},
   "source": [
    "### Testing ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fbaf3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([54.59081836, 15.05403516,  3.54571034]), 423.1747990944212)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_regression(y, tx , lambda_=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Some other functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def quantile_normalize(data, q=0.95):\n",
    "    low    = (1-q) / 2\n",
    "    high   = 1-low\n",
    "    q_low  = np.quantile(data, low,  axis=0)\n",
    "    q_high = np.quantile(data, high, axis=0)\n",
    "    median = np.quantile(data, 0.5, axis=0)\n",
    "    return (data - median) / (q_high - q_low)\n",
    "\n",
    "def mim_max_normalize(data):\n",
    "    return (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))\n",
    "\n",
    "def z_normalize(data):\n",
    "    return (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "\n",
    "def accuracy(y, tx, w):\n",
    "    pred    = np.where(implementation.sigmoid(tx.dot(w)) > 0.5, 1, 0)\n",
    "    correct = np.sum(np.where(pred == y, 1, 0))\n",
    "    return correct / len(y)\n",
    "\n",
    "def accuracy2(y, tx, w):\n",
    "    pred    = np.where(tx.dot(w) > 0, 1, 0)\n",
    "    correct = np.sum(np.where(pred == y, 1, 0))\n",
    "    return correct / len(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed=0):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "def cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indices = k_indices[k]\n",
    "    tr_indices = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indices = tr_indices.reshape(-1)\n",
    "    y_te = y[te_indices]\n",
    "    y_tr = y[tr_indices]\n",
    "    tx_te = x[te_indices]\n",
    "    tx_tr = x[tr_indices]\n",
    "\n",
    "    w, loss_tr = implementation.reg_logistic_regression(y_tr, tx_tr, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "    accuracy_te = accuracy(y_te, tx_te, w)\n",
    "    accuracy_tr = accuracy(y_tr, tx_tr, w)\n",
    "    return accuracy_tr, accuracy_te, w\n",
    "'''\n",
    "seed = 7\n",
    "k_fold = 5\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "max_iters = 2000\n",
    "gamma     = 0.000003\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "accuracy_tr = []\n",
    "accuracy_te = []\n",
    "# cross validation\n",
    "for lambda_ in lambdas:\n",
    "    rmse_tr_tmp = []\n",
    "    rmse_te_tmp = []\n",
    "    for k in range(k_fold):\n",
    "        loss_tr, loss_te,_ = cross_validation(y, tx, k_indices, k, lambda_)\n",
    "        rmse_tr_tmp.append(loss_tr)\n",
    "        rmse_te_tmp.append(loss_te)\n",
    "    accuracy_tr.append(np.mean(rmse_tr_tmp))\n",
    "    accuracy_te.append(np.mean(rmse_te_tmp))\n",
    "'''\n",
    "'''plt.semilogx(lambdas, accuracy_tr)\n",
    "plt.semilogx(lambdas, accuracy_te)\n",
    "\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('accuracy')'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_interaction_tx(input_data, normalisation_function):\n",
    "    input_data = normalisation_function(input_data)\n",
    "\n",
    "    n_features = input_data.shape[1]\n",
    "    n_interacted_features = int(n_features + (n_features-1) * n_features / 2)\n",
    "\n",
    "    x = np.empty((n_interacted_features, len(input_data)))\n",
    "    x[:n_features] = input_data.T\n",
    "    index = n_features\n",
    "    for i in range(n_features):\n",
    "        for j in range(i):\n",
    "            x[index] = x[i] * x[j]\n",
    "            index = index + 1\n",
    "\n",
    "    x = normalisation_function(x.T)\n",
    "    tx = np.append(np.ones(len(x)).reshape(-1,1), x, axis=1)\n",
    "\n",
    "    return tx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def GD(y, tx, initial_w, max_iters, gamma, gradient_func, loss_func):\n",
    "    w = initial_w\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        if i%500 == 0:\n",
    "            print(str(accuracy(y, tx, w)).ljust(25, ' '), accuracy(y_te, tx_te, w))\n",
    "        grad = gradient_func(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "\n",
    "    loss = loss_func(y, tx, w)\n",
    "    return w, loss\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    w, loss = GD(y, tx, initial_w, max_iters, gamma, implementation.logistic_gradient, implementation.logistic_loss)\n",
    "    return w, loss"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
