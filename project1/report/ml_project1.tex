


%\documentclass[10pt,conference,compsocconf]{IEEEtran}
\documentclass[sigconf, nonacm]{acmart}
%\usepackage{makecell}
% Disable some elements from ACM template
%\setcopyright{none}
%\settopmatter{printacmref=false,printfolios=false}
\usepackage{caption}

\usepackage{booktabs} % For formal tables

\usepackage{todonotes}
\begin{document}
\title{Project 1: Detecting Higgs boson}

\author{
  Rolando Grave de Peralta Gonzalez, Maxime Rufer\\
  \{rolando.gravedeperaltagonzalez, maxime.rufer\}@epfl.ch
  \\
  \textit{Department of Computer Science, EPF Lausanne, Switzerland}
}
\begin{abstract}
	%TODO : short abstract
\end{abstract}

\maketitle



\section{Introduction}
In March 2013, the Large Hadron Collider at CERN announced discovery of the Higgs boson particle. This is an elementary particle in the Standard Model of physics which explains why other particles have mass. The Higgs boson decays rapidly into other particles, this is the reason why it is rather detected by chain of products in which it decays. Scientists call this it's "decay signature". In this report we will be classifying wether a "decay signature" corresponds to that's of a Higgs boson or not.
\todo{Ajouter? Modif?}

%\section{Related Works}


\section{Methods}
\subsection{Data}

\subsubsection{Exploratory Data Analysis}
\paragraph{Data Statistical Properties}Before creating baselines and complex models, we explore the given data. When looking at the data we first check for missing or wrongly formatted data. Which we follow by analysing basic statistical properties of our dataset and we see that we have large means, ranging from $\approx -709 $ up to $\approx 210$. We also see that our data has some features with high variance, up to $\approx 658$. \todo{Add plots of mean/std ?}\\
\paragraph{Class Distribution} When looking at our dataset we can see a big imbalance in the representation of both classes. We have over 164'333 samples that are in class -1 and 85'667 which are in class 1. Which means that class -1 corresponds to roughly $65.7\%$ and class 1 to $34.3\%$ of all data. This can sometimes affect performance of some models, but as we will se later it doesn't. \\
\paragraph{Other?} Should we add graphs about the distributions? Boxplots? Violinplots? CorrelationPlot? Pairwise plots/hists?

\subsubsection{Data Pre-processing \& Engineering}
\paragraph*{}
\paragraph{Z-score} Since we have such large variances and means we will proceed to using z-score scaling as one of the proposed pre-processing methods. This method is implemented in the \textit{implementation.py} file as  \textit{z\_normalize()}
\paragraph{}
\paragraph{Z-score}
\paragraph{Interaction terms}
\subsubsection{Feature extraction} %TODO: POSSIBLY ADD FEATURE EXTRACTION?
- PCA? No tools to implement..
- 

\subsection{Evaluation and Baselines}

\section{Models}
\subsection{Baselines}
\subsubsection{Static Model} A simple model that can serve as baseline is one that doesn't need any feature and simply outputs the most represented class in it's training data.
\subsubsection{Bayesian Prior?}%TODO: CHECK SUGGESTION
\subsection{Least Squares}
\subsection{Logistic Regression}



\section{Results}
%\begin{table}[]
%	\begin{tabular}{lllll}
%		\hline
%		Model&  &  & Test Acc. &Eval. Acc.  \\ \hline
%		Static Model&  &  & 4 &1  \\ \hline
%		1&  &  &3  & 3 \\
%		Logistic Regression&  &  &  3&3  \\ \hline
%	\end{tabular}
%\end{table}
\subsection{Environment,Implementation and Training Details}
\paragraph{}We ran these experiments on consumer grade x86 CPUs. All machine learning tools are implemented by using numpy. This allowed us to fix randomness's seed using numpy's \text{np.random.seed}. We used the seed 42 all along the experience for to ensure reproducibility of the results. 
\subsection{Test Results}
\paragraph{} In this section we present the results of our proposed models and compare them with the baselines and among them-selfs. The accuracy scores we use to compare the different models is based on Cross Validation using 5-fold validation using the same seed and thus the same folds among models. This allows better generalisation of our results rather than arbitrary taking 1 test set and 1 training set, or worse training on the public leader-board.

\section{Discussion}
- speak about model performance why?
- speak about possible improvements to the model in terms of accuracy and runtime.(For the later one could use different gradient descent methods and modifications (Using weight intertia, adaptive learning rate etc..))

\section{Conclusion}
- Conclude on results, happy? Not Happy? Why? 
- Discuss possibility of implementing more complex problems but no time to code them? Use libraries?







%\bibliographystyle{IEEEtran}
%\bibliography{literature}

\end{document}
