{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments     # https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#transformers.TFTrainingArguments\n",
    "from transformers import Trainer    # https://huggingface.co/transformers/v3.0.2/main_classes/trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "\n",
    "from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import wandb\n",
    "\n",
    "# Config file\n",
    "from transformer_config import Configuration\n",
    "from transformer_config import CONSTANTS as C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/jtqmvsps2tv_1fbgs_k5vfkw0000gp/T/ipykernel_99032/393488888.py:10: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  METRIC = load_metric(\"accuracy\")    # metric to use\n"
     ]
    }
   ],
   "source": [
    "# Constant Values for Discord bot\n",
    "discord_hook= C.discord_hook\n",
    "bot_name = C.bot_name\n",
    "red = 15158332\n",
    "green = 3066993\n",
    "orange = 15105570\n",
    "\n",
    "\n",
    "# Global variables\n",
    "METRIC = load_metric(\"accuracy\")    # metric to use\n",
    "val_dataset_global = None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TWITTER_FOLDER = DATA_FOLDER + 'twitter-datasets/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# DATASET CLASSES\n",
    "class TrainDataset(Dataset):\n",
    "    '''Pytorch Dataset object used to store the data in a format\n",
    "    that can be easily sent to the gpu for the models.'''\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(value[idx]) for key, value in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)     # number of items in the Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    '''Pytorch Dataset object used to store the data in a format\n",
    "    that can be easily sent to the gpu for the models.'''\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(value[idx]) for key, value in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])     # number of items in the Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Can I supress this one ?\n",
    "if `config.use_HF_dataset_format` is always True, then we can supress this one"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Read a file containing tweets and store them as well as their respective labels\n",
    "def read_file_HF(file_name, starting_line=0, end_line=0):\n",
    "    '''Helper function to read a file with the HF format ( label \\t text).\n",
    "    takes the filename, the starting line and the ending line as parameters.\n",
    "    Will return the requested lines in 2 numpy arrays teets, labels.'''\n",
    "    with open( file_name, 'r', encoding='utf-8') as f:\n",
    "        tweets = [line.split(\"\\t\",maxsplit=1) for line in f.readlines()[starting_line:end_line]]\n",
    "        tweets = np.array(tweets)\n",
    "        labels = tweets[:,0].astype(np.uint8)\n",
    "        tweets = tweets[:,1]\n",
    "\n",
    "    return tweets, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Can I supress this one ?\n",
    "if `config.use_HF_dataset_format` is always True, then we can supress this one"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Store the training data, tweets and labels, in numpy arrays\n",
    "def load_train_data(amount_per_batch, iteration):\n",
    "    '''Handles reading the data in batches. Uses the helper function read_file_HF to do so.\n",
    "    takes as input: the amount of data per subset and the actual iteration number\n",
    "    and returns two arrays: tweets, labels  with length equal or lesser than amount_per_batch.'''\n",
    "\n",
    "    starting_line = iteration * amount_per_batch\n",
    "    end_line = starting_line + amount_per_batch\n",
    "    print(f\"Going to read {amount_per_batch*2} lines ({amount_per_batch} in each of the pos and neg datasets), starting_line:{starting_line}, end_line:{end_line}\")\n",
    "    tweets, labels = read_file_HF(project_path + \"HF_data.txt\", starting_line, end_line)\n",
    "    print(f\"Loaded {len(tweets)} tweets!\")\n",
    "\n",
    "\n",
    "    return tweets, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Store the testing data (tweets) in a numpy array\n",
    "def load_test_data():\n",
    "    '''Loads the testing data and returns it in a numpy array.'''\n",
    "    filename = project_path + \"test_data.txt\"\n",
    "    tweets = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.partition(\",\")[2]   # this allows to get the text content only for each line of the file (see that the file starts with \"n,\" where n is the line numbner); it parts the string into three strings as: before the arg, the arg, and after the arg\n",
    "            tweets.append(line.rstrip())\n",
    "\n",
    "    return tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Can I supress this one ?\n",
    "as `config.freq_words` is False in general"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Vectorize the data\n",
    "def vectorize_data(tweets, train_indices, val_indices):\n",
    "    '''Takes as input:\n",
    "    tweets: np.array of tweets in text mode.\n",
    "    train_indices: np.array of the train indices of the tweets array\n",
    "     val_indices: np array of the validation indices of the tweets array.\n",
    "     returns X_train,X_val in vectorized form using the CountVectorizer\n",
    "    '''\n",
    "    vectorizer = CountVectorizer(max_features=5000)   # 5000\n",
    "\n",
    "    # Important: we call fit_transform on the training set, and only transform on the validation set\n",
    "    X_train = vectorizer.fit_transform(tweets[train_indices])\n",
    "    X_val = vectorizer.transform(tweets[val_indices])\n",
    "\n",
    "    return X_train, X_val"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Can I supress this one ?\n",
    "if `config.use_HF_dataset_format` is always True, then we can supress this one"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_train_val_data(tweets, labels):\n",
    "    ''' Takes as input the tweets and labels as numpy arrays.\n",
    "    The tweets being still in text, it will tokenize it using the corresponding tokenizer of the model.\n",
    "\n",
    "    If it is the first call, it set ups the validation sets, and the training set.\n",
    "    On future calls, it reuses the first validation set, and uses the full tweets as training data.\n",
    "    it returns train_dataset: TrainDataset,val_dataset_global: TrainDataset\n",
    "\n",
    "    '''\n",
    "    global val_dataset_global\n",
    "    if val_dataset_global is None:\n",
    "        nb_of_samples = len(tweets)\n",
    "        shuffled_indices = np.random.permutation(nb_of_samples)\n",
    "        split_idx = int(train_val_ratio * nb_of_samples)\n",
    "\n",
    "        train_indices = shuffled_indices[:split_idx]\n",
    "        val_indices = shuffled_indices[split_idx:]\n",
    "\n",
    "        print(\"Number of indices for training: \", len(train_indices))\n",
    "        print(\"Number of indices for validation: \", len(val_indices))\n",
    "\n",
    "        if use_most_freq_words:\n",
    "            X_train, X_val = vectorize_data(tweets, train_indices, val_indices)\n",
    "        else:\n",
    "            X_train, X_val = tweets[train_indices], tweets[val_indices]\n",
    "\n",
    "        Y_train = labels[train_indices]\n",
    "        Y_val = labels[val_indices]\n",
    "\n",
    "        X_train = tokenizer(X_train.tolist(), max_length=config.tokenizer_max_length, padding=\"max_length\", truncation=True)\n",
    "        X_val = tokenizer(X_val.tolist(), max_length=config.tokenizer_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "        Y_train = torch.tensor(Y_train).clone().detach()\n",
    "        Y_val = torch.tensor(Y_val).clone().detach()\n",
    "\n",
    "        train_dataset = TrainDataset(X_train, Y_train)\n",
    "        val_dataset = TrainDataset(X_val, Y_val)\n",
    "        val_dataset_global = val_dataset\n",
    "\n",
    "        return train_dataset, val_dataset\n",
    "    else:\n",
    "        nb_of_samples = len(tweets)\n",
    "        shuffled_indices = np.random.permutation(nb_of_samples)\n",
    "\n",
    "        train_indices = shuffled_indices\n",
    "        print(\"Number of indices for training: \", len(train_indices))\n",
    "\n",
    "        if use_most_freq_words:\n",
    "            X_train = vectorize_data(tweets, train_indices, val_indices)\n",
    "        else:\n",
    "            X_train = tweets[train_indices]\n",
    "\n",
    "        Y_train = labels[train_indices]\n",
    "\n",
    "        X_train = tokenizer(X_train.tolist(), max_length=config.tokenizer_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "        Y_train = torch.tensor(Y_train).clone().detach()\n",
    "        train_dataset = TrainDataset(X_train, Y_train)\n",
    "\n",
    "        return train_dataset,val_dataset_global"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_test_data(tweets):\n",
    "    ''' Takes as input the tweets as a numpy array with The tweets still\n",
    "     being still in text, it will tokenize it using the corresponding tokenizer of the model.\n",
    "     The tokenizer_max_length must be set from the config either passing the corresponding parameter,\n",
    "     or the default value.\n",
    "     returns the TestDataset object.\n",
    "\n",
    "    '''\n",
    "    nb_of_samples = len(tweets)\n",
    "    print(f'{nb_of_samples} tweets loaded for testing.\\n')\n",
    "    tweets = tokenizer(tweets, max_length=config.tokenizer_max_length, padding=\"max_length\", truncation=True)\n",
    "    tweets = TestDataset(tweets)\n",
    "\n",
    "    return tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, train_dataset, val_dataset, iteration):\n",
    "    '''Helper function to start the training of a model.\n",
    "    takes as input:\n",
    "    - model: a huggingface model\n",
    "    - train_dataset: a TrainDataset object used for the training.\n",
    "    - val_dataset: a TrainDataset object used for the validation\n",
    "    - iteration : is used when running with subsets, to save the different models of different versions.\n",
    "\n",
    "    Creates a Trainer using the training arguments defaults, and the ones passed by commandline.\n",
    "    Doesn't return anything per se, but the model taken as parameter will be trained after the training.'''\n",
    "\n",
    "    training_args = TrainingArguments(output_dir=checkpoints_path,\n",
    "                                      overwrite_output_dir=True,\n",
    "                                      per_device_train_batch_size=config.bs_train,\n",
    "                                      per_device_eval_batch_size=config.bs_eval,\n",
    "                                      learning_rate=config.lr,\n",
    "                                      evaluation_strategy=\"steps\",            # \"steps\"\n",
    "                                      save_strategy=\"steps\",\n",
    "                                      gradient_accumulation_steps=4,\n",
    "                                      # gradient_checkpointing=True,\n",
    "                                      save_total_limit=2,\n",
    "                                      fp16=config.fp16,\n",
    "                                      seed=config.seed,\n",
    "                                      warmup_steps=500,                       # number of warmup steps for learning rate scheduler\n",
    "                                      weight_decay=config.weight_decay,       # strength of weight decay\n",
    "                                      logging_dir='./logs',                   # directory for storing logs\n",
    "                                      logging_steps=500,\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      num_train_epochs=config.n_epochs,\n",
    "                                      report_to=\"wandb\" # WANDB INTEGRATION\n",
    "                                      )\n",
    "\n",
    "\n",
    "    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        #   data_collator=data_collator,\n",
    "        #   tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    if len(os.listdir(checkpoints_path)) == 0:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        trainer.train(checkpoints_path, resume_from_checkpoint=True)    # ??? Working like that?\n",
    "\n",
    "    best_model_at_iteration_path = f\"/best_model/iteration{iteration}\"\n",
    "    trainer.save_model(experiments_results_path + best_model_at_iteration_path)    # save the best model of the current iteration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(path_to_checkpoint):\n",
    "    ''' Helper function, to load the model from a checkpoint.\n",
    "    takes as input a path to the checkpoint (from the \"experiment-[...]\" )\n",
    "     '''\n",
    "    full_path_to_model_checkpoint = experiment_path + path_to_checkpoint\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(full_path_to_model_checkpoint, num_labels=config.num_labels, local_files_only=False, ignore_mismatched_sizes=True)\n",
    "    print(f\"Loaded model from: {full_path_to_model_checkpoint}\")\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def numpy_2d_softmax(model_preds):\n",
    "    '''Converts the raw predictions from a HuggingFace model into clean logits.'''\n",
    "    max = np.max(model_preds, axis=1, keepdims=True)\n",
    "    e_x = np.exp(model_preds-max)\n",
    "    sum = np.sum(e_x, axis=1, keepdims=True)\n",
    "    out = e_x / sum\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is `logits` ??"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    ''' Takes the trained model as input, and will get the testing data and produce the predictions.\n",
    "    Also produces the logits.txt file that is used for ensembling the models.\n",
    "    returns a numpy array with the predictions of the model.\n",
    "     '''\n",
    "\n",
    "    test_trainer = Trainer(model)\n",
    "    tweets = get_test_data(load_test_data())\n",
    "    raw_preds, _, _ = test_trainer.predict(tweets)     # only predictions to return, no label ids, no metrics; see HF Trainer doc\n",
    "    Y_test_pred = np.argmax(raw_preds, axis=1)\n",
    "\n",
    "    # store the logits in a file\n",
    "    logits = numpy_2d_softmax(raw_preds)    # beer owning line\n",
    "    print(len(logits))\n",
    "    print(logits)\n",
    "\n",
    "    if not(config.model_name is None):\n",
    "        model_name_for_logits = config.model_name.split(\"/\")[1]\n",
    "    else:\n",
    "        model_name_for_logits = \"NoModelNameGiven\"\n",
    "\n",
    "    if not(config.load_model is None):\n",
    "        model_name_for_logits = config.load_model.split(\"experiment-\")[1].split(\"\\\\\")[0]\n",
    "\n",
    "    np.savetxt(test_results_path + model_name_for_logits + \"-\" + 'logits.txt', logits, delimiter=\",\", header = \"negative,positive\", comments = \"\") # fmt=\"%1d\"\n",
    "\n",
    "    return Y_test_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_submission(Y_preds):\n",
    "    '''Takes as input a numpy array containing the model predictions, and generates\n",
    "    a correctly formatted output csv file for the kaggle competition.'''\n",
    "    nb_of_samples=len(Y_preds)\n",
    "    results = np.zeros((nb_of_samples, 2))\n",
    "\n",
    "    results[:,0] = np.arange(1, nb_of_samples+1).astype(np.int32)  # save the ids\n",
    "    results[:,1] = [-1 if elem == 0 else 1 for elem in Y_preds]  # save the test predictions\n",
    "\n",
    "    final_filename = f\"{experiment_date_for_folder_name}-submission.csv\"\n",
    "    np.savetxt(test_results_path + final_filename, results, fmt=\"%1d\", delimiter=\",\", header = \"Id,Prediction\", comments = \"\")\n",
    "\n",
    "    return final_filename"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Supress this one later ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DISCORD\n",
    "def send_discord_notif(title, content, color, error=None):\n",
    "    '''Simple function to allow callbacks to the discord channel to get updates when running for a long time.'''\n",
    "    if not discord_enabled:\n",
    "        return\n",
    "    if error is None:\n",
    "        msg = \"Little Update on my status\"\n",
    "    else:\n",
    "        msg = error\n",
    "\n",
    "    data = {\n",
    "        \"username\": bot_name,\n",
    "        \"content\": msg,\n",
    "    }\n",
    "    data[\"embeds\"] = [{\n",
    "        \"title\":title,\n",
    "        \"description\":content,\n",
    "        \"color\":color\n",
    "    }]\n",
    "\n",
    "    result = requests.post(discord_hook, json = data)\n",
    "\n",
    "    try:\n",
    "        result.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(err)\n",
    "    else:\n",
    "        print(\"Payload delivered successfully, code {}.\".format(result.status_code))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prints for debug only ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "def run_training(model):\n",
    "    ''' Function to call from the main to start the training, takes as input the model to train.\n",
    "    It will handle all the requirements for the training.\n",
    "    It will load all the data required for training, if using subsets it will handle all the iterations over the data.\n",
    "    If enabled it will send discord notification on training start, and at each subset ending.\n",
    "    It will also handle the wandb initialisation of a project with name being: cil-{model_name}\n",
    "    It will also catch errors that happened during training and send them to the discord hook and proceed to exit.\n",
    "\n",
    "    returns a trained model. '''\n",
    "    print(f\"Going to iterate over {number_of_iterations} subsets of {amount_per_it} samples/tweets (separated for training/validation) to see {total_amount_of_tweets} in total.\")\n",
    "    send_discord_notif(\"Starting Training\", f\"Going to iterate over {number_of_iterations} subsets of {amount_per_it} samples/tweets (separated for training/validation) to see {total_amount_of_tweets} in total.\", orange, None)\n",
    "    wandb_project_name = f\"cil-{model_name}\".replace(\"/\",\"-\").replace(\"\\\\\",\"\").replace(\"?\",\"\").replace(\"%\",\"\").replace(\":\",\"\")\n",
    "    wandb.init(project=wandb_project_name)\n",
    "    try:\n",
    "        total_subsets = range(number_of_iterations)[config.start_at_it:]\n",
    "\n",
    "        if config.use_HF_dataset_format:\n",
    "            total_subsets = range(1)\n",
    "\n",
    "        for iteration in total_subsets:\n",
    "            trained_model = load_and_train(model, amount_per_it, iteration)\n",
    "            # torch.cuda.empty_cache()  # can be used if save the trained model before that line and load it again after that line\n",
    "            send_discord_notif(\"Continuing Training\", f\"currently finished subset iteration: {iteration+1}/{number_of_iterations} \", orange, None)\n",
    "            print(f\"{iteration+1} out of {number_of_iterations} subset iteration(s) done!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"GOT ERROR:\", str(e))\n",
    "        send_discord_notif(\"ERROR WHILE TRAINING\", str(e), red, f\"Got the error at subset iteration: {iteration+1}/{number_of_iterations}\")\n",
    "        raise(e)\n",
    "\n",
    "    send_discord_notif(\"Finished Training\", f\"Used {number_of_iterations} subsets of {amount_per_it} samples (i.e., tweets) without problem. Used {total_amount_of_tweets} in total.\", green, None)\n",
    "    print(f\"Finished Training. Used {number_of_iterations} subsets of {amount_per_it} samples (i.e., tweets) without problem. Used {total_amount_of_tweets} tweets in total.\\n\")\n",
    "\n",
    "    return trained_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TESTING\n",
    "def run_testing(model):\n",
    "    '''Function to call from main to start the testing of a trained model.\n",
    "    Takes as input the trained model and returns the output filename.csv'''\n",
    "    try:\n",
    "        Y_test_pred = test(model)\n",
    "        submit_filename = generate_submission(Y_test_pred)\n",
    "    except Exception as e:\n",
    "        print(\"GOT ERROR:\", str(e))\n",
    "        send_discord_notif(\"ERROR WHILE TESTING\", str(e), red, f\"Got the error while trying to predict on test set.\")\n",
    "        raise(e)\n",
    "\n",
    "    send_discord_notif(\"Finished to predict on test set\", f\"Everything ran without issues!\", green, None)\n",
    "\n",
    "    return submit_filename"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_and_train(model, amount_per_batch, iteration):\n",
    "    '''Helper function for training the model using the batches strategie to allow the model the run on systems with low amount of memory.\n",
    "    takes as input:\n",
    "    -model: the model to train (HF model)\n",
    "    -amount_per_batch: The amount of data to be loaded and used on each iteration.\n",
    "    -iteration: At which iteration the training is.\n",
    "\n",
    "    In case the use HF_dataset format has been used, you should not run this function multiple times, only once as\n",
    "    all the data will be loaded using HuggingFace's api. There is also no way to select the amount of data to work\n",
    "    on when using this parameter.\n",
    "\n",
    "\n",
    "    returns the trained model.'''\n",
    "\n",
    "    if model is None:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(config.model_name, num_labels=config.num_labels, local_files_only=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "    if config.use_HF_dataset_format: # Using HuggingFace api to load all the data.\n",
    "\n",
    "        datasets = load_dataset(\"./HF_dataset.py\")\n",
    "\n",
    "        def tokenization(sample):\n",
    "            return tokenizer(sample[\"text\"], max_length=config.tokenizer_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "        datasets = datasets.map(tokenization, batched=True)\n",
    "\n",
    "        train_dataset = datasets[\"train\"]\n",
    "        val_dataset = datasets[\"validation\"]\n",
    "\n",
    "    else:\n",
    "        # Load training & validation data\n",
    "        tweets, labels = load_train_data(amount_per_batch, iteration)\n",
    "        train_dataset, val_dataset = get_train_val_data(tweets, labels)\n",
    "        tweets, labels = [], []\n",
    "\n",
    "        # Free some memory\n",
    "        del(tweets)\n",
    "        del(labels)\n",
    "\n",
    "    # TRAINING\n",
    "    train(model, train_dataset, val_dataset, iteration)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # To time the duration of the experiment\n",
    "    time_run = time.time()     # better to use perf_counter() than time()\n",
    "\n",
    "    # Get the config\n",
    "    config = Configuration.parse_cmd()\n",
    "\n",
    "    # Prepare the folder where this experiment (i.e., program run) outputs and results will be saved\n",
    "    experiment_id = int(time_run)\n",
    "    experiment_date = time.ctime(experiment_id)\n",
    "    print(\"CURRENT DATE TIME: \", experiment_date)\n",
    "    experiment_date_name = experiment_date.replace(\" \", \"_\").replace(\":\", \"h\")[:-8] + experiment_date[-8:-5].replace(\":\", \"m\") + \"s\"\n",
    "    experiment_date_for_folder_name = \"experiment-\" + experiment_date_name\n",
    "\n",
    "    # Prepare and set the paths\n",
    "    if config.on_cluster:\n",
    "        print(\"\\nRunning on the cluster.\")\n",
    "        project_path = os.environ[\"CIL_PROJECT_PATH\"]   # see cluster .bashrc file for the environment variables\n",
    "        experiment_path = os.environ[\"CIL_EXPERIMENTS_PATH\"] + \"Experiments/\"   # see cluster .bashrc file for the environment variables\n",
    "    else:\n",
    "        print(\"\\nRunning locally.\")\n",
    "        project_path = \"./\"\n",
    "        experiment_path = \"./\" + \"Experiments/\"\n",
    "\n",
    "    experiments_results_path = experiment_path + experiment_date_for_folder_name\n",
    "    os.makedirs(experiments_results_path, exist_ok=True)    # create the experiment folder(s) needed\n",
    "    checkpoints_path = experiments_results_path + \"/checkpoints/\"\n",
    "    print(\"The project path is: \", project_path)\n",
    "    print(\"The experiment path is: \", experiment_path)\n",
    "    print(\"The model checkpoints will be saved at: \", checkpoints_path, \"\\n\")\n",
    "\n",
    "    # for the submission\n",
    "    test_results_path = experiments_results_path + \"/test_results/\"\n",
    "    os.makedirs(test_results_path, exist_ok=True)    # create the folder(s) if needed\n",
    "\n",
    "\n",
    "    # Fix seeds for reproducibility\n",
    "    SEED = config.seed\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True # DEBUG: comment this when debugging.\n",
    "\n",
    "    # Save in the experiment folder the command line that was used to run this program\n",
    "    cmd = sys.argv[0] + ' ' + ' '.join(sys.argv[1:])\n",
    "    with open(os.path.join(experiments_results_path, 'cmd.txt'), 'w') as f:\n",
    "        f.write(cmd)\n",
    "\n",
    "    # Data\n",
    "    use_full_dataset = config.full_data\n",
    "    model_name = config.model_name\n",
    "    use_most_freq_words = config.freq_words\n",
    "    train_val_ratio = config.train_val_ratio\n",
    "\n",
    "    # Calculate values for the number of tweets to work with during model training\n",
    "    amount_per_it = config.amount_per_it\n",
    "    full_dataset_size = 2500000\n",
    "    small_dataset_size = 200000\n",
    "\n",
    "    if config.amount_of_data > small_dataset_size:\n",
    "        use_full_dataset = True\n",
    "\n",
    "    if use_full_dataset:\n",
    "        total_amount_of_tweets = full_dataset_size    # max number of tweets there are in the full dataset\n",
    "    else:\n",
    "        total_amount_of_tweets = small_dataset_size     # max number of tweets there are in the size-reduced dataset\n",
    "\n",
    "    # if we don't want to use the complete dataset (whether the full one or the size-reduced one)\n",
    "    if config.amount_of_data != 0:\n",
    "        total_amount_of_tweets = config.amount_of_data\n",
    "        if total_amount_of_tweets > full_dataset_size:\n",
    "            total_amount_of_tweets = full_dataset_size\n",
    "\n",
    "    number_of_iterations = int(np.ceil(total_amount_of_tweets / amount_per_it))\n",
    "    print(f\"We will use {total_amount_of_tweets} tweets in total. {int(train_val_ratio * total_amount_of_tweets)} for training and {int(total_amount_of_tweets * (1-train_val_ratio))} for validation.\")\n",
    "    print(f\"{amount_per_it} tweets will be used for each of the {number_of_iterations} subset iterations (i.e., in each subset that is split in training/validation).\")\n",
    "\n",
    "    # Misc\n",
    "    submit_to_kaggle = config.autosubmit\n",
    "    discord_enabled = config.discord\n",
    "\n",
    "    # Model\n",
    "    n_epochs = config.n_epochs\n",
    "    bs_train = config.bs_train\n",
    "    bs_eval = config.bs_eval\n",
    "    lr = config.lr\n",
    "    fp16 = config.fp16\n",
    "    weight_decay = config.weight_decay\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    # Create the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=config.num_labels, local_files_only=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "    # If we need to load a model from a checkpoint or not\n",
    "    if not (config.load_model is None):\n",
    "        with open(experiment_path + config.load_model + \"/config.json\", 'r') as json_file:\n",
    "            json_dict = json.load(json_file)\n",
    "\n",
    "        model_name = json_dict[\"_name_or_path\"]\n",
    "        print(\"Using a checkpoint from the model architecture: \", model_name)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        model = load_model_from_checkpoint(config.load_model)   # load_model should be (from a previous exp) e.g.: experiment-Thu_Jul_28_03h29m56s/checkpoints/checkpoint-14500\n",
    "\n",
    "    model.to(C.DEVICE)  # automatic if use the Trainer()\n",
    "    print(\"\\nRunning on\", C.DEVICE, \" with PyTorch\", torch.__version__, \"\\n\")\n",
    "\n",
    "    # --- TRAINING & VALIDATION ---\n",
    "    if config.train:\n",
    "        model = run_training(model)\n",
    "\n",
    "    # --- TESTING ---\n",
    "    if config.test:\n",
    "        submit_filename = run_testing(model)\n",
    "\n",
    "    # Time that took the whole experiment to run\n",
    "    time_run = time.time() - time_run\n",
    "    print(f\"The program took {str(time_run/60/60)[:6]} Hours or {str(time_run/60)[:6]} minutes to run.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
