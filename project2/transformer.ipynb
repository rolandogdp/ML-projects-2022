{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments     # https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#transformers.TFTrainingArguments\n",
    "from transformers import Trainer    # https://huggingface.co/transformers/v3.0.2/main_classes/trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "\n",
    "from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import wandb\n",
    "\n",
    "# Config file\n",
    "from transformer_config import Configuration\n",
    "from transformer_config import CONSTANTS as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8267/2222091020.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  METRIC = load_metric(\"accuracy\")    # metric to use\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Global variables\n",
    "METRIC = load_metric(\"accuracy\")    # metric to use\n",
    "val_dataset_global = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TWITTER_FOLDER = DATA_FOLDER + 'twitter-datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c30d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred): \n",
    "    '''Required function to evaluate the predictions of the model on the evaluation dataset.\n",
    "     '''\n",
    "    logits, labels = eval_pred      # here, we have to get rid of the second element (neutral class) of the logits before taking the softmax IF we want to only predict neg/pos\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return METRIC.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DATASET CLASSES\n",
    "class TrainDataset(Dataset):\n",
    "    '''Pytorch Dataset object used to store the data in a format\n",
    "    that can be easily sent to the gpu for the models.'''\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.long()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(value[idx]) for key, value in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)     # number of items in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    '''Pytorch Dataset object used to store the data in a format\n",
    "    that can be easily sent to the gpu for the models.'''\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(value[idx]) for key, value in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])     # number of items in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store the testing data (tweets) in a numpy array\n",
    "def load_test_data():\n",
    "    '''Loads the testing data and returns it in a numpy array.'''\n",
    "    filename = project_path + \"test_data.txt\"\n",
    "    tweets = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.partition(\",\")[2]   # this allows to get the text content only for each line of the file (see that the file starts with \"n,\" where n is the line numbner); it parts the string into three strings as: before the arg, the arg, and after the arg\n",
    "            tweets.append(line.rstrip())\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_test_data(tweets):\n",
    "    ''' Takes as input the tweets as a numpy array with The tweets still\n",
    "     being still in text, it will tokenize it using the corresponding tokenizer of the model.\n",
    "     The tokenizer_max_length must be set from the config either passing the corresponding parameter,\n",
    "     or the default value.\n",
    "     returns the TestDataset object.\n",
    "\n",
    "    '''\n",
    "    nb_of_samples = len(tweets)\n",
    "    print(f'{nb_of_samples} tweets loaded for testing.\\n')\n",
    "    tweets = tokenizer(tweets, max_length=config.tokenizer_max_length, padding=\"max_length\", truncation=True)\n",
    "    tweets = TestDataset(tweets)\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataset, val_dataset):\n",
    "    '''Helper function to start the training of a model.\n",
    "    takes as input:\n",
    "    - model: a huggingface model\n",
    "    - train_dataset: a TrainDataset object used for the training.\n",
    "    - val_dataset: a TrainDataset object used for the validation\n",
    "\n",
    "    Creates a Trainer using the training arguments defaults, and the ones passed by commandline.\n",
    "    Doesn't return anything per se, but the model taken as parameter will be trained after the training.'''\n",
    "\n",
    "    training_args = TrainingArguments(output_dir=checkpoints_path,\n",
    "                                      overwrite_output_dir=True,\n",
    "                                      per_device_train_batch_size=config.bs_train,\n",
    "                                      per_device_eval_batch_size=config.bs_eval,\n",
    "                                      learning_rate=config.lr,\n",
    "                                      evaluation_strategy=\"steps\",            # \"steps\"\n",
    "                                      save_strategy=\"steps\",\n",
    "                                      gradient_accumulation_steps=4,\n",
    "                                      # gradient_checkpointing=True,\n",
    "                                      save_total_limit=2,\n",
    "                                      fp16=config.fp16,\n",
    "                                      seed=config.seed,\n",
    "                                      warmup_steps=500,                       # number of warmup steps for learning rate scheduler\n",
    "                                      weight_decay=config.weight_decay,       # strength of weight decay\n",
    "                                      logging_dir='./logs',                   # directory for storing logs\n",
    "                                      logging_steps=500,\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      num_train_epochs=config.n_epochs,\n",
    "                                      report_to=\"wandb\" # WANDB INTEGRATION\n",
    "                                      )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    if len(os.listdir(checkpoints_path)) == 0:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        trainer.train(checkpoints_path, resume_from_checkpoint=True)    # ??? Working like that?\n",
    "\n",
    "    best_model_at_end_path = f\"/best_model/\"\n",
    "    trainer.save_model(experiments_results_path + best_model_at_end_path)    # save the best model of the current iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(path_to_checkpoint):\n",
    "    ''' Helper function, to load the model from a checkpoint.\n",
    "    takes as input a path to the checkpoint (from the \"experiment-[...]\" )\n",
    "     '''\n",
    "    full_path_to_model_checkpoint = experiment_path + path_to_checkpoint\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(full_path_to_model_checkpoint, num_labels=config.num_labels, local_files_only=False, ignore_mismatched_sizes=True)\n",
    "    print(f\"Loaded model from: {full_path_to_model_checkpoint}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numpy_2d_softmax(model_preds):\n",
    "    '''Converts the raw predictions from a HuggingFace model into clean logits.'''\n",
    "    max = np.max(model_preds, axis=1, keepdims=True)\n",
    "    e_x = np.exp(model_preds-max)\n",
    "    sum = np.sum(e_x, axis=1, keepdims=True)\n",
    "    out = e_x / sum\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What is `logits` ??\n",
    "logits could be interpreted as the probability predicted by the model. This was/is used when combining different models, as it yields better results to average these values instead of the rounded predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    ''' Takes the trained model as input, and will get the testing data and produce the predictions.\n",
    "    Also produces the logits.txt file that is used for ensembling the models.\n",
    "    returns a numpy array with the predictions of the model.\n",
    "     '''\n",
    "\n",
    "    test_trainer = Trainer(model)\n",
    "    tweets = get_test_data(load_test_data())\n",
    "    raw_preds, _, _ = test_trainer.predict(tweets)     # only predictions to return, no label ids, no metrics; see HF Trainer doc\n",
    "    Y_test_pred = np.argmax(raw_preds, axis=1)\n",
    "\n",
    "    # store the logits in a file\n",
    "    logits = numpy_2d_softmax(raw_preds)    # beer owning line\n",
    "    print(len(logits))\n",
    "    print(logits)\n",
    "\n",
    "    if not(config.model_name is None):\n",
    "        model_name_for_logits = config.model_name.split(\"/\")[1]\n",
    "    else:\n",
    "        model_name_for_logits = \"NoModelNameGiven\"\n",
    "\n",
    "    if not(config.load_model is None):\n",
    "        model_name_for_logits = config.load_model.split(\"experiment-\")[1].split(\"\\\\\")[0]\n",
    "\n",
    "    np.savetxt(test_results_path + model_name_for_logits + \"-\" + 'logits.txt', logits, delimiter=\",\", header = \"negative,positive\", comments = \"\") # fmt=\"%1d\"\n",
    "\n",
    "    return Y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_submission(Y_preds):\n",
    "    '''Takes as input a numpy array containing the model predictions, and generates\n",
    "    a correctly formatted output csv file for the kaggle competition.'''\n",
    "    nb_of_samples=len(Y_preds)\n",
    "    results = np.zeros((nb_of_samples, 2))\n",
    "\n",
    "    results[:,0] = np.arange(1, nb_of_samples+1).astype(np.int32)  # save the ids\n",
    "    results[:,1] = [-1 if elem == 0 else 1 for elem in Y_preds]  # save the test predictions\n",
    "\n",
    "    final_filename = f\"{experiment_date_for_folder_name}-submission.csv\"\n",
    "    np.savetxt(test_results_path + final_filename, results, fmt=\"%1d\", delimiter=\",\", header = \"Id,Prediction\", comments = \"\")\n",
    "\n",
    "    return final_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Prints for debug only ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "def run_training(model):\n",
    "    ''''''\n",
    "    wandb_project_name = f\"ML-epfl-{model_name}\".replace(\"/\",\"-\").replace(\"\\\\\",\"\").replace(\"?\",\"\").replace(\"%\",\"\").replace(\":\",\"\")\n",
    "    wandb.init(project=wandb_project_name)\n",
    "    try:\n",
    "        print(\"Starting load and train\")\n",
    "        trained_model = load_and_train(model)\n",
    "        # torch.cuda.empty_cache()  # can be used if save the trained model before that line and load it again after that line\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"GOT ERROR:\", str(e))\n",
    "        raise(e)\n",
    "\n",
    "    print(f\"Finished Training without problem.\\n\")\n",
    "\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "def run_prediction(model):\n",
    "    '''Function to call from main to start the testing of a trained model.\n",
    "    Takes as input the trained model and returns the output filename.csv'''\n",
    "    try:\n",
    "        Y_test_pred = predict(model)\n",
    "        submit_filename = generate_submission(Y_test_pred)\n",
    "    except Exception as e:\n",
    "        print(\"GOT ERROR:\", str(e))\n",
    "        raise(e)\n",
    "\n",
    "    return submit_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_and_train(model):\n",
    "    '''Helper function for training the model using the batches strategie to allow the model the run on systems with low amount of memory.\n",
    "    takes as input:\n",
    "    -model: the model to train (HF model)\n",
    "    \n",
    "    returns the trained model.'''\n",
    "\n",
    "    if model is None:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(config.model_name, num_labels=config.num_labels, local_files_only=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    datasets = load_dataset(\"./HF_dataset.py\")\n",
    "\n",
    "    def tokenization(sample):\n",
    "        return tokenizer(sample[\"text\"], max_length=config.tokenizer_max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    datasets = datasets.map(tokenization, batched=True)\n",
    "\n",
    "    train_dataset = datasets[\"train\"]\n",
    "    val_dataset = datasets[\"validation\"]\n",
    "\n",
    "    # TRAINING\n",
    "    train(model, train_dataset, val_dataset)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--tag TAG] [--seed SEED] [--on_cluster]\n",
      "                             [--autosubmit] [--discord]\n",
      "                             [--load_model LOAD_MODEL] [--test] [--train]\n",
      "                             [--num_labels NUM_LABELS] [--full_data]\n",
      "                             [--amount_of_data AMOUNT_OF_DATA]\n",
      "                             [--amount_per_it AMOUNT_PER_IT]\n",
      "                             [--start_at_it START_AT_IT]\n",
      "                             [--use_HF_dataset_format] [--freq_words]\n",
      "                             [--tokenizer_max_length TOKENIZER_MAX_LENGTH]\n",
      "                             [--model_name MODEL_NAME]\n",
      "                             [--train_val_ratio TRAIN_VAL_RATIO] [--lr LR]\n",
      "                             [--n_epochs N_EPOCHS]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--bs_train BS_TRAIN] [--bs_eval BS_EVAL]\n",
      "                             [--fp16]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/home/user/.local/share/jupyter/runtime/kernel-v2-73138YA0VEtaJv6n.json could match --full_data, --freq_words, --fp16\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/ethz/semester3-epfl/MachineLearning/ML-projects-2022/project1/.new-venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3386: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # To time the duration of the experiment\n",
    "    time_run = time.time()     # better to use perf_counter() than time()\n",
    "\n",
    "    # Get the config\n",
    "    config = Configuration.parse_cmd()\n",
    "\n",
    "    # Prepare the folder where this experiment (i.e., program run) outputs and results will be saved\n",
    "    experiment_id = int(time_run)\n",
    "    experiment_date = time.ctime(experiment_id)\n",
    "    print(\"CURRENT DATE TIME: \", experiment_date)\n",
    "    experiment_date_name = experiment_date.replace(\" \", \"_\").replace(\":\", \"h\")[:-8] + experiment_date[-8:-5].replace(\":\", \"m\") + \"s\"\n",
    "    experiment_date_for_folder_name = \"experiment-\" + experiment_date_name\n",
    "\n",
    "    # Prepare and set the paths\n",
    "    if config.on_cluster:\n",
    "        print(\"\\nRunning on the cluster.\")\n",
    "        project_path = os.environ[\"CIL_PROJECT_PATH\"]   # see cluster .bashrc file for the environment variables\n",
    "        experiment_path = os.environ[\"CIL_EXPERIMENTS_PATH\"] + \"Experiments/\"   # see cluster .bashrc file for the environment variables\n",
    "    else:\n",
    "        print(\"\\nRunning locally.\")\n",
    "        project_path = \"./\"\n",
    "        experiment_path = \"./\" + \"Experiments/\"\n",
    "\n",
    "    experiments_results_path = experiment_path + experiment_date_for_folder_name\n",
    "    os.makedirs(experiments_results_path, exist_ok=True)    # create the experiment folder(s) needed\n",
    "    checkpoints_path = experiments_results_path + \"/checkpoints/\"\n",
    "    print(\"The project path is: \", project_path)\n",
    "    print(\"The experiment path is: \", experiment_path)\n",
    "    print(\"The model checkpoints will be saved at: \", checkpoints_path, \"\\n\")\n",
    "\n",
    "    # for the submission\n",
    "    test_results_path = experiments_results_path + \"/test_results/\"\n",
    "    os.makedirs(test_results_path, exist_ok=True)    # create the folder(s) if needed\n",
    "\n",
    "\n",
    "    # Fix seeds for reproducibility\n",
    "    SEED = config.seed\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True # DEBUG: comment this when debugging.\n",
    "\n",
    "    # Save in the experiment folder the command line that was used to run this program\n",
    "    cmd = sys.argv[0] + ' ' + ' '.join(sys.argv[1:])\n",
    "    with open(os.path.join(experiments_results_path, 'cmd.txt'), 'w') as f:\n",
    "        f.write(cmd)\n",
    "\n",
    "    # Data\n",
    "    \n",
    "    model_name = config.model_name\n",
    "    use_most_freq_words = config.freq_words\n",
    "    train_val_ratio = config.train_val_ratio\n",
    "\n",
    "    discord_enabled = config.discord\n",
    "\n",
    "    # Model\n",
    "    n_epochs = config.n_epochs\n",
    "    bs_train = config.bs_train\n",
    "    bs_eval = config.bs_eval\n",
    "    lr = config.lr\n",
    "    fp16 = config.fp16\n",
    "    weight_decay = config.weight_decay\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    # Create the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=config.num_labels, local_files_only=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "    # If we need to load a model from a checkpoint or not\n",
    "    if not (config.load_model is None):\n",
    "        with open(experiment_path + config.load_model + \"/config.json\", 'r') as json_file:\n",
    "            json_dict = json.load(json_file)\n",
    "\n",
    "        model_name = json_dict[\"_name_or_path\"]\n",
    "        print(\"Using a checkpoint from the model architecture: \", model_name)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        model = load_model_from_checkpoint(config.load_model)   # load_model should be (from a previous exp) e.g.: experiment-Thu_Jul_28_03h29m56s/checkpoints/checkpoint-14500\n",
    "\n",
    "    model.to(C.DEVICE)  # automatic if use the Trainer()\n",
    "    print(\"\\nRunning on\", C.DEVICE, \" with PyTorch\", torch.__version__, \"\\n\")\n",
    "\n",
    "    # --- TRAINING & VALIDATION ---\n",
    "    if config.train:\n",
    "        model = run_training(model)\n",
    "\n",
    "    # --- TESTING ---\n",
    "    if config.test:\n",
    "        submit_filename = run_prediction(model)\n",
    "\n",
    "    # Time that took the whole experiment to run\n",
    "    time_run = time.time() - time_run\n",
    "    print(f\"The program took {str(time_run/60/60)[:6]} Hours or {str(time_run/60)[:6]} minutes to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.new-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "698af77b4c07e8153daf17e5e6512c2599614a79691474a911f58b252a74c180"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
