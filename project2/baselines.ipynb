{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "## Test Maxime ML"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "#NLP libraries\n",
    "import spacy, nltk, gensim, sklearn\n",
    "\n",
    "#Vader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Scikit imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def count_parenthesis(tweet):\n",
    "    count = 0\n",
    "    for c in tweet:\n",
    "        if c == '(':\n",
    "            count += 1\n",
    "        if c == ')':\n",
    "            count -= 1\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# importing Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TWITTER_FOLDER = DATA_FOLDER + 'twitter-datasets/'\n",
    "EN_CORE_WEB_SM = DATA_FOLDER + 'en_core_web_sm-3.0.0'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "pos = pd.read_csv(TWITTER_FOLDER + 'train_pos_full.txt', sep='\\r\\t', header=None, names=['tweet'], engine='python')\n",
    "neg = pd.read_csv(TWITTER_FOLDER + 'train_neg_full.txt', sep='\\r\\t', header=None, names=['tweet'], engine='python', on_bad_lines='skip')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(TWITTER_FOLDER + 'test_data.txt', sep='\\t', header=None, names=['tweet'])\n",
    "test_data['tweet'] = test_data['tweet'].apply(lambda t : t[t.find(',')+1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating Test/Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adding some processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def combine_pos_neg(pos, neg, n=None):\n",
    "    pos = pos.drop_duplicates()[:n]\n",
    "    neg = neg.drop_duplicates()[:n]\n",
    "    pos['is_pos'] = 1\n",
    "    neg['is_pos'] = 0\n",
    "    return pd.concat([pos, neg])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def adding_metadata(tweets):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    polarity_scores = tweets['tweet'].apply(analyzer.polarity_scores)\n",
    "\n",
    "    tweets['neg']       = polarity_scores.apply(lambda d : d['neg'])\n",
    "    tweets['neu']       = polarity_scores.apply(lambda d : d['neu'])\n",
    "    tweets['pos']       = polarity_scores.apply(lambda d : d['pos'])\n",
    "    tweets['compound']  = polarity_scores.apply(lambda d : d['compound'])\n",
    "\n",
    "    tweets['par_count'] = tweets[\"tweet\"].apply(count_parenthesis)\n",
    "    tweets['len_tweet'] = tweets[\"tweet\"].apply(len)\n",
    "    return tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Split test / train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def split_test_train(tweets):\n",
    "    X = tweets.drop(['is_pos'], axis=1)\n",
    "    Y = tweets.drop(['tweet'],  axis=1)\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    X_tr_df, X_te_df, Y_tr_df, Y_te_df = train_test_split(X, Y, test_size=10000, random_state=42)\n",
    "    return X_tr_df, X_te_df, Y_tr_df, Y_te_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vectorization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def vectorize_tweets(vectorizer, X_tr_df, X_te_df):\n",
    "    X_tr_vec = vectorizer.fit_transform(X_tr_df['tweet'])\n",
    "    X_te_vec = vectorizer.transform(X_te_df['tweet'])\n",
    "    return X_tr_vec, X_te_vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def scale_data(scaler, X_tr, X_te):\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_te = scaler.transform(X_te)\n",
    "    return X_tr, X_te"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_scaled_data(scaler, features):\n",
    "    X_tr                     = X_tr_df[features].to_numpy()\n",
    "    X_te                     = X_te_df[features].to_numpy()\n",
    "    X_tr_scaled, X_te_scaled = scale_data(scaler, X_tr, X_te)\n",
    "    return X_tr_scaled, X_te_scaled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PreProcessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "tweets = combine_pos_neg(pos, neg)\n",
    "tweets = adding_metadata(tweets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "X_tr_df, X_te_df, Y_tr_df, Y_te_df = split_test_train(tweets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.75, ngram_range=(1,5), strip_accents='unicode')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "Y_tr = np.array(Y_tr_df['is_pos'])\n",
    "Y_te = np.array(Y_te_df['is_pos'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "par_count_tr,        par_count_te        = get_scaled_data(scaler, ['par_count'])\n",
    "pol_score_tr,        pol_score_te        = get_scaled_data(scaler, ['neg', 'neu', 'pos', 'compound'])\n",
    "metadata_tr,         metadata_te         = get_scaled_data(scaler, ['neg', 'neu', 'pos', 'compound', 'par_count', 'len_tweet'])\n",
    "\n",
    "\n",
    "X_tr_vec,            X_te_vec            = vectorize_tweets(vectorizer, X_tr_df, X_te_df)\n",
    "X_tr_with_pol_score, X_te_with_pol_score = sparse.hstack((X_tr_vec, pol_score_tr)), sparse.hstack((X_te_vec, pol_score_te))\n",
    "X_tr_with_par_cont,  X_te_with_par_cont  = sparse.hstack((X_tr_vec, par_count_tr)), sparse.hstack((X_te_vec, par_count_te))\n",
    "X_tr_with_metadata,  X_te_with_metadata  = sparse.hstack((X_tr_vec, metadata_tr)),  sparse.hstack((X_te_vec, metadata_te))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def fit_and_get_score(model, X_tr, X_te, Y_tr=Y_tr, Y_te=Y_te):\n",
    "    model.fit(X_tr, Y_tr)\n",
    "    acc_tr = model.score(X_tr, Y_tr)\n",
    "    acc_te = model.score(X_te, Y_te)\n",
    "    return acc_tr, acc_te"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "svc     = LinearSVC()\n",
    "rfc     = RandomForestClassifier(max_depth=100, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "pol_score_acc      = fit_and_get_score( log_reg,  pol_score_tr,        pol_score_te)\n",
    "par_count_acc      = fit_and_get_score( log_reg,  par_count_tr,        par_count_te)\n",
    "metadata_acc       = fit_and_get_score( log_reg,  metadata_tr,         metadata_te)\n",
    "X_tr_vec_acc       = fit_and_get_score( log_reg,  X_tr_vec,            X_te_vec)\n",
    "with_pol_score_acc = fit_and_get_score( log_reg,  X_tr_with_pol_score, X_te_with_pol_score)\n",
    "with_par_cont_acc  = fit_and_get_score( log_reg,  X_tr_with_par_cont,  X_te_with_par_cont)\n",
    "with_metadata_acc  = fit_and_get_score( log_reg,  X_tr_with_metadata,  X_te_with_metadata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10000, 2222156]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [46]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m svc_acc            \u001B[38;5;241m=\u001B[39m \u001B[43mfit_and_get_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43msvc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m      \u001B[49m\u001B[43mX_tr_vec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m            \u001B[49m\u001B[43mX_tr_vec\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m rfc_acc            \u001B[38;5;241m=\u001B[39m fit_and_get_score( rfc,      X_tr_vec,            X_tr_vec)\n",
      "Input \u001B[0;32mIn [38]\u001B[0m, in \u001B[0;36mfit_and_get_score\u001B[0;34m(model, X_tr, X_te, Y_tr, Y_te)\u001B[0m\n\u001B[1;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_tr, Y_tr)\n\u001B[1;32m      3\u001B[0m acc_tr \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mscore(X_tr, Y_tr)\n\u001B[0;32m----> 4\u001B[0m acc_te \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_te\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_te\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m acc_tr, acc_te\n",
      "File \u001B[0;32m~/miniconda3/envs/untitled/lib/python3.9/site-packages/sklearn/base.py:666\u001B[0m, in \u001B[0;36mClassifierMixin.score\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    641\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    642\u001B[0m \u001B[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001B[39;00m\n\u001B[1;32m    643\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    662\u001B[0m \u001B[38;5;124;03m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001B[39;00m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    664\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score\n\u001B[0;32m--> 666\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43maccuracy_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/untitled/lib/python3.9/site-packages/sklearn/metrics/_classification.py:211\u001B[0m, in \u001B[0;36maccuracy_score\u001B[0;34m(y_true, y_pred, normalize, sample_weight)\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;124;03m\"\"\"Accuracy classification score.\u001B[39;00m\n\u001B[1;32m    146\u001B[0m \n\u001B[1;32m    147\u001B[0m \u001B[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;124;03m0.5\u001B[39;00m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;66;03m# Compute accuracy for each possible representation\u001B[39;00m\n\u001B[0;32m--> 211\u001B[0m y_type, y_true, y_pred \u001B[38;5;241m=\u001B[39m \u001B[43m_check_targets\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    212\u001B[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_type\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultilabel\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/untitled/lib/python3.9/site-packages/sklearn/metrics/_classification.py:84\u001B[0m, in \u001B[0;36m_check_targets\u001B[0;34m(y_true, y_pred)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_targets\u001B[39m(y_true, y_pred):\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \n\u001B[1;32m     60\u001B[0m \u001B[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;124;03m    y_pred : array or indicator matrix\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 84\u001B[0m     \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     type_true \u001B[38;5;241m=\u001B[39m type_of_target(y_true, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     86\u001B[0m     type_pred \u001B[38;5;241m=\u001B[39m type_of_target(y_pred, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_pred\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/untitled/lib/python3.9/site-packages/sklearn/utils/validation.py:387\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[0;34m(*arrays)\u001B[0m\n\u001B[1;32m    385\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 387\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    388\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[1;32m    390\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [10000, 2222156]"
     ]
    }
   ],
   "source": [
    "svc_acc            = fit_and_get_score( svc,      X_tr_vec,            X_tr_vec)\n",
    "rfc_acc            = fit_and_get_score( rfc,      X_tr_vec,            X_tr_vec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test/train for prediction using logistic regression with :\n",
      "only polarity score                      (0.644183851054775, 0.6413)\n",
      "only parenthesis count                   (0.6553398788399996, 0.6524)\n",
      "only metadata                            (0.737303371581813, 0.7372)\n",
      "only vectorized tweet                    (0.8782626891078982, 0.8433)\n",
      "vectorized tweet plus polarity score     (0.8782626891078982, 0.8449)\n",
      "vectorized tweet plus parenthesis count  (0.8856726131860373, 0.8548)\n",
      "vectorized tweet plus tweet metadata     (0.8855266266221098, 0.8547)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy test/train for prediction using logistic regression with :')\n",
    "print('only polarity score                     ',pol_score_acc)\n",
    "print('only parenthesis count                  ',par_count_acc)\n",
    "print('only metadata                           ',metadata_acc)\n",
    "print('only vectorized tweet                   ',X_tr_vec_acc)\n",
    "print('vectorized tweet plus polarity score    ',with_pol_score_acc)\n",
    "print('vectorized tweet plus parenthesis count ',with_par_cont_acc)\n",
    "print('vectorized tweet plus tweet metadata    ',with_metadata_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('accuracy test/train for prediction using')\n",
    "print('Linear Support Vector Machine classifier', svc_acc)\n",
    "print('Random Forest classifier                ', rfc_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deeper Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "X_tr = sparse.csr_matrix(X_tr_with_metadata)\n",
    "X_te = sparse.csr_matrix(X_te_with_metadata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test/train for prediction using logistic regression on the subset of tweets with positive parenthesis count : \n",
      "when trained on the corresponding subset (0.9824, 0.9816)\n",
      "when trained on the whole dataset        (0.8855, 0.9834)\n"
     ]
    }
   ],
   "source": [
    "subset_acc = fit_and_get_score(log_reg, X_tr[X_tr_df.par_count > 0], X_te[X_te_df.par_count > 0],\n",
    "                                        Y_tr[Y_tr_df.par_count > 0], Y_te[Y_te_df.par_count > 0])\n",
    "\n",
    "full_acc   = fit_and_get_score(log_reg, X_tr, X_te[X_te_df.par_count > 0],\n",
    "                                        Y_tr, Y_te[Y_te_df.par_count > 0])\n",
    "\n",
    "print('Accuracy test/train for prediction using logistic regression on the subset of tweets with positive parenthesis count : ')\n",
    "print('when trained on the corresponding subset ({:.4}, {:.4})'.format(subset_acc[0], subset_acc[1]))\n",
    "print('when trained on the whole dataset        ({:.4}, {:.4})'.format(full_acc[0],   full_acc[1]  ))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test/train for prediction using logistic regression on the subset of tweets with negative parenthesis count : \n",
      "when trained on the corresponding subset (0.9031, 0.896)\n",
      "when trained on the whole dataset        (0.8855, 0.8893)\n"
     ]
    }
   ],
   "source": [
    "subset_acc = fit_and_get_score(log_reg, X_tr[X_tr_df.par_count < 0], X_te[X_te_df.par_count < 0],\n",
    "                                        Y_tr[Y_tr_df.par_count < 0], Y_te[Y_te_df.par_count < 0])\n",
    "\n",
    "full_acc   = fit_and_get_score(log_reg, X_tr, X_te[X_te_df.par_count < 0],\n",
    "                                        Y_tr, Y_te[Y_te_df.par_count < 0])\n",
    "\n",
    "print('Accuracy test/train for prediction using logistic regression on the subset of tweets with negative parenthesis count : ')\n",
    "print('when trained on the corresponding subset ({:.4}, {:.4})'.format(subset_acc[0], subset_acc[1]))\n",
    "print('when trained on the whole dataset        ({:.4}, {:.4})'.format(full_acc[0],   full_acc[1]  ))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test/train for prediction using logistic regression on the subset of tweets with balanced parenthesis count : \n",
      "when trained on the corresponding subset (0.8654, 0.8288)\n",
      "when trained on the whole dataset        (0.8855, 0.8264)\n"
     ]
    }
   ],
   "source": [
    "subset_acc = fit_and_get_score(log_reg, X_tr[X_tr_df.par_count == 0], X_te[X_te_df.par_count == 0],\n",
    "                                        Y_tr[Y_tr_df.par_count == 0], Y_te[Y_te_df.par_count == 0])\n",
    "\n",
    "full_acc   = fit_and_get_score(log_reg, X_tr, X_te[X_te_df.par_count == 0],\n",
    "                                        Y_tr, Y_te[Y_te_df.par_count == 0])\n",
    "\n",
    "print('Accuracy test/train for prediction using logistic regression on the subset of tweets with balanced parenthesis count : ')\n",
    "print('when trained on the corresponding subset ({:.4}, {:.4})'.format(subset_acc[0], subset_acc[1]))\n",
    "print('when trained on the whole dataset        ({:.4}, {:.4})'.format(full_acc[0],   full_acc[1]  ))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "log_reg.fit(X_tr[X_tr_df.par_count > 0], Y_tr[Y_tr_df.par_count > 0]);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "test_data_df = adding_metadata(test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "test_data_vec = vectorizer.transform(test_data_df['tweet'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "scaler.fit(X_te_df[['neg', 'neu', 'pos', 'compound', 'par_count', 'len_tweet']])\n",
    "test_metadata = scaler.transform(test_data_df[['neg', 'neu', 'pos', 'compound', 'par_count', 'len_tweet']])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "test_with_metadata  = sparse.hstack((test_data_vec, test_metadata))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "test_data_df['pred'] = log_reg.predict(test_with_metadata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "translation = test_data_df[test_data_df.par_count > 0]['pred'].to_dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "with open('dict.txt', 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(translation))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
