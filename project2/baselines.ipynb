{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Baselines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "#Vader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Scikit imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## importing Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "TWITTER_FOLDER = DATA_FOLDER + 'twitter-datasets/'\n",
    "EN_CORE_WEB_SM = DATA_FOLDER + 'en_core_web_sm-3.0.0'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "pos = pd.read_csv(TWITTER_FOLDER + 'train_pos_full.txt', sep='\\r\\t', header=None, names=['tweet'], engine='python')\n",
    "neg = pd.read_csv(TWITTER_FOLDER + 'train_neg_full.txt', sep='\\r\\t', header=None, names=['tweet'], engine='python', on_bad_lines='skip')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "test_data_raw = pd.read_csv(TWITTER_FOLDER + 'test_data.txt', sep='\\t', header=None, names=['tweet'])\n",
    "\n",
    "test_data = pd.DataFrame()\n",
    "test_data['Id'] = test_data_raw['tweet'].apply(lambda t : t[:t.find(',')])\n",
    "test_data['tweet'] = test_data_raw['tweet'].apply(lambda t : t[t.find(',')+1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining useful methods for per-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def split_test_train(tweets, test_size=10000):\n",
    "    \"\"\"\n",
    "    Split the labelled tweets in train/test set. Randomisation for the creation is fixed for reproducibility.\n",
    "\n",
    "    :param tweets: Labelled tweets in a Dataframe\n",
    "    :param test_size: The size of the test set\n",
    "    :return: dataset of tweet splited in train/test set\n",
    "    \"\"\"\n",
    "    X = tweets.drop(['is_pos'], axis=1)\n",
    "    Y = tweets.drop(['tweet'],  axis=1)\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    X_tr_df, X_te_df, Y_tr_df, Y_te_df = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "    return X_tr_df, X_te_df, Y_tr_df, Y_te_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def combine_pos_neg(pos, neg):\n",
    "    \"\"\"\n",
    "    :param pos: Dataframe containing positive tweets\n",
    "    :param neg: Dataframe containing negative tweets\n",
    "    :return: the combination of the two sets with labels in one Dataframe (without duplicates)\n",
    "    \"\"\"\n",
    "    pos['is_pos'] = 1\n",
    "    neg['is_pos'] = 0\n",
    "    return pd.concat([pos, neg]).drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def count_parenthesis(tweet):\n",
    "    \"\"\"\n",
    "    :param tweet: a tweet\n",
    "    :return: the count of opening parenthesis minus the closing parenthesis\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for c in tweet:\n",
    "        if c == '(':\n",
    "            count += 1\n",
    "        if c == ')':\n",
    "            count -= 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def adding_metadata(tweets):\n",
    "    \"\"\"\n",
    "    Add metadata to the dataframe of tweet : the added metadata are the result of sentiment analysis and the count of parenthesis\n",
    "    :param tweets: A Dataframe of tweet\n",
    "    :return: The same dataframe with additional metadata\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # we keep track on the time to apply the sentiment analysis to the dataset\n",
    "    start_time = time.time()\n",
    "    polarity_scores = tweets['tweet'].apply(analyzer.polarity_scores)\n",
    "    sentiment_analysis_time = time.time() - start_time\n",
    "    print('Time to apply sentiment analysis on the dataset {:.4} seconde'.format(sentiment_analysis_time))\n",
    "\n",
    "    # storing the result of the sentiment analysis\n",
    "    tweets['neg']       = polarity_scores.apply(lambda d : d['neg'])\n",
    "    tweets['neu']       = polarity_scores.apply(lambda d : d['neu'])\n",
    "    tweets['pos']       = polarity_scores.apply(lambda d : d['pos'])\n",
    "    tweets['compound']  = polarity_scores.apply(lambda d : d['compound'])\n",
    "\n",
    "    # storing the parenthesis count\n",
    "    tweets['par_count'] = tweets[\"tweet\"].apply(count_parenthesis)\n",
    "    return tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def scale_data(scaler, X_tr, X_te):\n",
    "    \"\"\"\n",
    "    Scales the dataset according to the given scaler, The scaler is fitted only with the training set.\n",
    "    :param scaler: The scaler used for the scaling\n",
    "    :param X_tr: training set\n",
    "    :param X_te: testing set\n",
    "    :return: scaled version of the training/testing set\n",
    "    \"\"\"\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_te = scaler.transform(X_te)\n",
    "    return X_tr, X_te\n",
    "\n",
    "def get_scaled_data(scaler, X_tr_df, X_te_df, features):\n",
    "    \"\"\"\n",
    "    Scales the dataset with the features given according to the given scaler, the scaler is fitted only with the training set.\n",
    "    :param scaler: The scaler used for the scaling\n",
    "    :param X_tr: training set in Dataframe\n",
    "    :param X_te: testing set in Dataframe\n",
    "    :param features: features of the dataset we want to scale\n",
    "    :return: scaled version of the training/testing set with only the corresponding features.\n",
    "    \"\"\"\n",
    "    X_tr                     = X_tr_df[features].to_numpy()\n",
    "    X_te                     = X_te_df[features].to_numpy()\n",
    "    X_tr_scaled, X_te_scaled = scale_data(scaler, X_tr, X_te)\n",
    "    return X_tr_scaled, X_te_scaled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def vectorize_tweets(vectorizer, X_tr_df, X_te_df):\n",
    "    \"\"\"\n",
    "    Returns vectorized tweets with the given vectorizer, the vectoriser is only fitted on the training dataset\n",
    "    :param vectorizer: The vectorizer used for vectorization\n",
    "    :param X_tr_df: the training set in Dataframe\n",
    "    :param X_te_df: the testing set in Dataframe\n",
    "    :return: the vectorized test/train tweets with the given vectorizer\n",
    "    \"\"\"\n",
    "    X_tr_vec = vectorizer.fit_transform(X_tr_df['tweet'])\n",
    "    X_te_vec = vectorizer.transform(X_te_df['tweet'])\n",
    "    return X_tr_vec, X_te_vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PreProcessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to apply sentiment analysis on the dataset 116.9 seconde\n"
     ]
    }
   ],
   "source": [
    "tweets = combine_pos_neg(pos, neg)\n",
    "tweets = adding_metadata(tweets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "X_tr_df, X_te_df, Y_tr_df, Y_te_df = split_test_train(tweets, test_size=10000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# vectorize the tweets\n",
    "start_time = time.time()\n",
    "vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1,5), strip_accents='unicode')\n",
    "X_tr_vec,            X_te_vec            = vectorize_tweets(vectorizer, X_tr_df, X_te_df)\n",
    "vectorization_time = time.time() - start_time\n",
    "\n",
    "# scale the metadata\n",
    "scaler = StandardScaler()\n",
    "par_count_tr,        par_count_te        = get_scaled_data(scaler, X_tr_df, X_te_df, ['par_count'])\n",
    "pol_score_tr,        pol_score_te        = get_scaled_data(scaler, X_tr_df, X_te_df, ['neg', 'neu', 'pos', 'compound'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to vectorize the dataset 220.3 seconde\n"
     ]
    }
   ],
   "source": [
    "print('Time to vectorize the dataset {:.4} seconde'.format(vectorization_time))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# get the labels\n",
    "Y_tr = np.array(Y_tr_df['is_pos'])\n",
    "Y_te = np.array(Y_te_df['is_pos'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def create_inputs(has_vectorized_tweet, has_par_count, has_pol_score):\n",
    "    \"\"\"\n",
    "    Combine the features into one sparse matrix.\n",
    "    :param has_vectorized_tweet: Boolean if the de desired input has the vectorized tweet.\n",
    "    :param has_par_count: Boolean if the de desired input has the parenthesis count.\n",
    "    :param has_pol_score: Boolean if the de desired input has the polarity score.\n",
    "    :return: the combined train/test features into one sparse matrix.\n",
    "    \"\"\"\n",
    "    X_tr = None\n",
    "    X_te = None\n",
    "\n",
    "    if has_par_count:\n",
    "        X_tr = sparse.hstack((X_tr, par_count_tr))\n",
    "        X_te = sparse.hstack((X_te, par_count_te))\n",
    "    if has_pol_score:\n",
    "        X_tr = sparse.hstack((X_tr, pol_score_tr))\n",
    "        X_te = sparse.hstack((X_te, pol_score_te))\n",
    "    if has_vectorized_tweet:\n",
    "        X_tr = sparse.hstack((X_tr, X_tr_vec))\n",
    "        X_te = sparse.hstack((X_te, X_te_vec))\n",
    "\n",
    "    return sparse.csr_matrix(X_tr), sparse.csr_matrix(X_te)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we define a function that test and store results for each runs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Constant for getting the full training set\n",
    "FULL_TRAIN_SET = [True]*len(X_tr_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def fit_and_store_res(df_score, model, model_name, has_vectorized_tweet, has_par_count, has_pol_score,\n",
    "                      subset_train=FULL_TRAIN_SET, subset_train_name='Full train set'):\n",
    "    \"\"\"\n",
    "\n",
    "    :param df_score: The dataframe in which we store the score for each run\n",
    "    :param model: The model used\n",
    "    :param model_name: The string name of the model\n",
    "    :param has_vectorized_tweet: Boolean if the de desired input has the vectorized tweet.\n",
    "    :param has_par_count: Boolean if the de desired input has the parenthesis count.\n",
    "    :param has_pol_score: Boolean if the de desired input has the polarity score.\n",
    "    :param subset_train: The subset in which the model will be trained\n",
    "    :param subset_train_name: The string name of the subset\n",
    "    :return: The updated dataframe with the result of the run\n",
    "    \"\"\"\n",
    "    X_tr, X_te = create_inputs(has_vectorized_tweet, has_par_count, has_pol_score)\n",
    "\n",
    "    # we monitor time for fitting the model\n",
    "    start_time     = time.time()\n",
    "    model.fit(X_tr[subset_train], Y_tr[subset_train])\n",
    "    time_to_fit     = time.time() - start_time\n",
    "\n",
    "    # we monitor the time for predicting the result\n",
    "    start_time      = time.time()\n",
    "    test_acc        = model.score(X_te, Y_te)\n",
    "    time_to_predict = time.time() - start_time\n",
    "\n",
    "    # we create a new row and store each value for the result\n",
    "    new_row = pd.DataFrame()\n",
    "\n",
    "    new_row['model']                         = model_name,\n",
    "    new_row['Train has vectorized tweet']    = has_vectorized_tweet,\n",
    "    new_row['Train has parenthesis count']   = has_par_count,\n",
    "    new_row['Train has polarity score']      = has_pol_score,\n",
    "    new_row['Train on']                      = subset_train_name,\n",
    "    new_row['time to fit [s]']               = time_to_fit,\n",
    "    new_row['time_to_predict [s]']           = time_to_predict,\n",
    "    new_row['accuracy on the training set']  = model.score(X_tr[subset_train], Y_tr[subset_train]),\n",
    "    new_row['accuracy on the full test set'] = test_acc,\n",
    "    new_row['accuracy on subset test 1']     = model.score(X_te[X_te_df.par_count == 0], Y_te[Y_te_df.par_count == 0]),\n",
    "    new_row['accuracy on subset test 2']     = model.score(X_te[X_te_df.par_count >  0], Y_te[Y_te_df.par_count >  0]),\n",
    "    new_row['accuracy on subset test 3']     = model.score(X_te[X_te_df.par_count <  0], Y_te[Y_te_df.par_count <  0])\n",
    "\n",
    "    return pd.concat([df_score, new_row])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running the predicitons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# definition of our four models tested for baselines\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "svc     = LinearSVC()\n",
    "rfc     = RandomForestClassifier(max_depth=10, random_state=42)\n",
    "bnb     = BernoulliNB()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# creating the dataframe of scores\n",
    "df_score = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# testing logistic regression with multiple combination of features for training\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', False, False, True )\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', False, True,  False)\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', True,  False, False)\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', False, True,  True )\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', True,  True,  False)\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', True,  False, True )\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', True,  True,  True )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# testing logistic regression with multiple combination of subset\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', True,  True,  True ,\n",
    "                             subset_train = X_tr_df.par_count == 0, subset_train_name= 'Subset 1')\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', True,  True,  True ,\n",
    "                             subset_train = X_tr_df.par_count >  0, subset_train_name= 'Subset 2')\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', True,  True,  True ,\n",
    "                             subset_train = X_tr_df.par_count <  0, subset_train_name= 'Subset 3')\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', True,  True,  True ,\n",
    "                             subset_train = X_tr_df.par_count >= 0, subset_train_name= 'Subset 1 and 2')\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', True,  True,  True ,\n",
    "                             subset_train = X_tr_df.par_count <= 0, subset_train_name= 'Subset 1 and 3')\n",
    "df_score = fit_and_store_res(df_score, log_reg, 'Logistic Regression', True,  True,  True ,\n",
    "                             subset_train = X_tr_df.par_count != 0, subset_train_name= 'Subset 2 and 3')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Maxime/miniconda3/envs/untitled/lib/python3.9/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# testing other models\n",
    "df_score = fit_and_store_res(df_score, svc,    'Support Vector Machine', True,  True,  True )\n",
    "df_score = fit_and_store_res(df_score, rfc,    'Random forest',          True,  True,  True )\n",
    "df_score = fit_and_store_res(df_score, bnb,    'Bernoulli',              True,  True,  True )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# storing the data\n",
    "df_score.to_csv(DATA_FOLDER + 'baseline_scores.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                    model  Train has vectorized tweet  \\\n0     Logistic Regression                       False   \n0     Logistic Regression                       False   \n0     Logistic Regression                        True   \n0     Logistic Regression                       False   \n0     Logistic Regression                        True   \n0     Logistic Regression                        True   \n0     Logistic Regression                        True   \n0     Logistic Regression                        True   \n0     Logistic Regression                        True   \n0     Logistic Regression                        True   \n0     Logistic Regression                        True   \n0     Logistic Regression                        True   \n0     Logistic Regression                        True   \n0  Support Vector Machine                        True   \n0           Random forest                        True   \n0               Bernoulli                        True   \n\n   Train has parenthesis count  Train has polarity score        Train on  \\\n0                        False                      True  Full train set   \n0                         True                     False  Full train set   \n0                        False                     False  Full train set   \n0                         True                      True  Full train set   \n0                         True                     False  Full train set   \n0                        False                      True  Full train set   \n0                         True                      True  Full train set   \n0                         True                      True        Subset 1   \n0                         True                      True        Subset 2   \n0                         True                      True        Subset 3   \n0                         True                      True  Subset 1 and 2   \n0                         True                      True  Subset 1 and 3   \n0                         True                      True  Subset 2 and 3   \n0                         True                      True  Full train set   \n0                         True                      True  Full train set   \n0                         True                      True  Full train set   \n\n   time to fit [s]  time_to_predict [s]  accuracy on the training set  \\\n0         1.371146             0.001740                      0.644184   \n0         1.086416             0.000961                      0.655340   \n0        33.288939             0.005042                      0.878264   \n0         1.748650             0.000876                      0.738640   \n0        32.675965             0.005268                      0.885723   \n0        61.178934             0.006363                      0.878325   \n0        74.933091             0.006318                      0.885728   \n0        55.337550             0.005222                      0.865341   \n0         8.603601             0.003918                      0.981464   \n0         2.322908             0.003162                      0.900728   \n0        85.340799             0.006346                      0.885138   \n0        65.128419             0.008166                      0.866694   \n0        11.696063             0.002945                      0.970328   \n0       665.704211             0.004632                      0.940804   \n0        45.391681             0.240420                      0.723708   \n0         2.668426             0.063346                      0.819482   \n\n   accuracy on the full test set  accuracy on subset test 1  \\\n0                         0.6413                   0.639107   \n0                         0.6524                   0.577503   \n0                         0.8432                   0.820801   \n0                         0.7378                   0.684000   \n0                         0.8548                   0.826537   \n0                         0.8447                   0.822048   \n0                         0.8565                   0.828657   \n0                         0.7159                   0.829031   \n0                         0.6712                   0.607058   \n0                         0.5193                   0.606934   \n0                         0.8549                   0.827410   \n0                         0.7358                   0.830403   \n0                         0.7725                   0.724529   \n0                         0.8463                   0.817558   \n0                         0.7215                   0.683377   \n0                         0.7969                   0.759446   \n\n   accuracy on subset test 2  accuracy on subset test 3  \n0                   0.664290                   0.570470  \n0                   0.969697                   0.875839  \n0                   0.957813                   0.798658  \n0                   0.969697                   0.875839  \n0                   0.983363                   0.889262  \n0                   0.958408                   0.812081  \n0                   0.983363                   0.889262  \n0                   0.151515                   0.859060  \n0                   0.980986                   0.647651  \n0                   0.036245                   0.889262  \n0                   0.980986                   0.882550  \n0                   0.259655                   0.879195  \n0                   0.980986                   0.885906  \n0                   0.976827                   0.882550  \n0                   0.920974                   0.620805  \n0                   0.959002                   0.889262  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>Train has vectorized tweet</th>\n      <th>Train has parenthesis count</th>\n      <th>Train has polarity score</th>\n      <th>Train on</th>\n      <th>time to fit [s]</th>\n      <th>time_to_predict [s]</th>\n      <th>accuracy on the training set</th>\n      <th>accuracy on the full test set</th>\n      <th>accuracy on subset test 1</th>\n      <th>accuracy on subset test 2</th>\n      <th>accuracy on subset test 3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>Full train set</td>\n      <td>1.371146</td>\n      <td>0.001740</td>\n      <td>0.644184</td>\n      <td>0.6413</td>\n      <td>0.639107</td>\n      <td>0.664290</td>\n      <td>0.570470</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>Full train set</td>\n      <td>1.086416</td>\n      <td>0.000961</td>\n      <td>0.655340</td>\n      <td>0.6524</td>\n      <td>0.577503</td>\n      <td>0.969697</td>\n      <td>0.875839</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Full train set</td>\n      <td>33.288939</td>\n      <td>0.005042</td>\n      <td>0.878264</td>\n      <td>0.8432</td>\n      <td>0.820801</td>\n      <td>0.957813</td>\n      <td>0.798658</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Full train set</td>\n      <td>1.748650</td>\n      <td>0.000876</td>\n      <td>0.738640</td>\n      <td>0.7378</td>\n      <td>0.684000</td>\n      <td>0.969697</td>\n      <td>0.875839</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>Full train set</td>\n      <td>32.675965</td>\n      <td>0.005268</td>\n      <td>0.885723</td>\n      <td>0.8548</td>\n      <td>0.826537</td>\n      <td>0.983363</td>\n      <td>0.889262</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>Full train set</td>\n      <td>61.178934</td>\n      <td>0.006363</td>\n      <td>0.878325</td>\n      <td>0.8447</td>\n      <td>0.822048</td>\n      <td>0.958408</td>\n      <td>0.812081</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Full train set</td>\n      <td>74.933091</td>\n      <td>0.006318</td>\n      <td>0.885728</td>\n      <td>0.8565</td>\n      <td>0.828657</td>\n      <td>0.983363</td>\n      <td>0.889262</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Subset 1</td>\n      <td>55.337550</td>\n      <td>0.005222</td>\n      <td>0.865341</td>\n      <td>0.7159</td>\n      <td>0.829031</td>\n      <td>0.151515</td>\n      <td>0.859060</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Subset 2</td>\n      <td>8.603601</td>\n      <td>0.003918</td>\n      <td>0.981464</td>\n      <td>0.6712</td>\n      <td>0.607058</td>\n      <td>0.980986</td>\n      <td>0.647651</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Subset 3</td>\n      <td>2.322908</td>\n      <td>0.003162</td>\n      <td>0.900728</td>\n      <td>0.5193</td>\n      <td>0.606934</td>\n      <td>0.036245</td>\n      <td>0.889262</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Subset 1 and 2</td>\n      <td>85.340799</td>\n      <td>0.006346</td>\n      <td>0.885138</td>\n      <td>0.8549</td>\n      <td>0.827410</td>\n      <td>0.980986</td>\n      <td>0.882550</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Subset 1 and 3</td>\n      <td>65.128419</td>\n      <td>0.008166</td>\n      <td>0.866694</td>\n      <td>0.7358</td>\n      <td>0.830403</td>\n      <td>0.259655</td>\n      <td>0.879195</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Subset 2 and 3</td>\n      <td>11.696063</td>\n      <td>0.002945</td>\n      <td>0.970328</td>\n      <td>0.7725</td>\n      <td>0.724529</td>\n      <td>0.980986</td>\n      <td>0.885906</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Support Vector Machine</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Full train set</td>\n      <td>665.704211</td>\n      <td>0.004632</td>\n      <td>0.940804</td>\n      <td>0.8463</td>\n      <td>0.817558</td>\n      <td>0.976827</td>\n      <td>0.882550</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Random forest</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Full train set</td>\n      <td>45.391681</td>\n      <td>0.240420</td>\n      <td>0.723708</td>\n      <td>0.7215</td>\n      <td>0.683377</td>\n      <td>0.920974</td>\n      <td>0.620805</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Bernoulli</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>Full train set</td>\n      <td>2.668426</td>\n      <td>0.063346</td>\n      <td>0.819482</td>\n      <td>0.7969</td>\n      <td>0.759446</td>\n      <td>0.959002</td>\n      <td>0.889262</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating submission for the leaderboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fitting our best model to the training set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "X_tr, X_te = create_inputs(True, True, True)\n",
    "log_reg.fit(X_tr, Y_tr);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pre-processing for the testing set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to apply sentiment analysis on the dataset 0.6682 seconde\n"
     ]
    }
   ],
   "source": [
    "test_data_df = adding_metadata(test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# vectorize the tweets\n",
    "test_data_vec = vectorizer.transform(test_data['tweet'])\n",
    "\n",
    "# scale the metadata\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr_df[['par_count', 'neg', 'neu', 'pos', 'compound']])\n",
    "test_metadata = scaler.transform(test_data_df[['par_count', 'neg', 'neu', 'pos', 'compound']])\n",
    "X_test = sparse.hstack((test_metadata, test_data_vec))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prediciton for the testing set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "test_data['Prediction'] = log_reg.predict(X_test)\n",
    "test_data['Prediction'] = test_data['Prediction'].apply(lambda p : 1 if p==1 else -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the submission"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "test_data[['Id', 'Prediction']].to_csv(DATA_FOLDER + 'prediction_baseline.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
